{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TFIDF sans repetition sans stem ---\n",
      "\n",
      "0.11142377892985601\n",
      "\n",
      "--- TFIDF avec repetition sans stem ---\n",
      "\n",
      "0.12598313294016977\n",
      "\n",
      "--- TFIDF sans repetition avec stem ---\n",
      "\n",
      "0.12945796222453965\n",
      "\n",
      "--- TFIDF avec repetition avec stem ---\n",
      "\n",
      "0.14711282134750892\n",
      "\n",
      "--- TFIDF avec repetition avec stem qu'avec les 10 ---\n",
      "\n",
      "0.18399999999999994\n",
      "\n",
      "--- TFIDF query 6 avec stem ---\n",
      "\n",
      "[(1357, 0.7756522981344043), (312, 0.7063976286581182), (1077, 0.699262299075713), (326, 0.6688595904202472), (325, 0.6593044534142437), (666, 0.6593044534142437), (698, 0.6180979250758535), (1289, 0.5993676849220397), (1295, 0.5993676849220397), (398, 0.549420377845203)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#stopwords is a list of useless words when it comes to query, words like the or is will appear several times and even tho their weight will be reduced accordingly, it is better to remove them\n",
    "stopwords = ['ourselves','hers','between','yourself','but','again','there','about','once','during','out','very','having','with',\n",
    "'they','own','an','be','some','for','do','its','yours','such','into','of','most','itself','other','off','is','s',\n",
    "'am','or','who','as','from','him','each','the','themselves','until','below','are','we','these','your','his','through',\n",
    "'don','nor','me','were','her','more','himself','this','down','should','our','their','while','above','both','up','to',\n",
    "'ours','had','she','all','no','when','at','any','before','them','same','and','been','have','in','will','on','does',\n",
    "'yourselves','then','that','because','what','over','why','so','can','did','not','now','under','he','you','herself','has',\n",
    "'just','where','too','only','myself','which','those','i','after','few','whom','t','being','if','theirs','my','against','a',\n",
    "'by','doing','it','how','further','was','here','than']\n",
    "\n",
    "def stemming(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "def clean_doc(liste):\n",
    "    # using list comprehension to perform the task, this function is removing cap letter and removing character like ? or ,\n",
    "    # This version of the function doesn't use stemming\n",
    "    res = []\n",
    "    for word in liste :\n",
    "        new_word = word.lower()\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', new_word)\n",
    "        if new_word != \"\" and new_word not in stopwords:\n",
    "            res.append(new_word)\n",
    "    return res\n",
    "\n",
    "def clean_doc_stem(liste):\n",
    "    # using list comprehension to perform the task, this function is removing cap letter and removing character like ? or ,\n",
    "    # This version of the function use stemming\n",
    "    res = []\n",
    "    for word in liste :\n",
    "        new_word = word.lower()\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', new_word)\n",
    "        if new_word != \"\" and new_word not in stopwords:\n",
    "            new_word_stem = stemming(new_word)\n",
    "            res.append(new_word_stem)\n",
    "    return res\n",
    "\n",
    "\n",
    "def indexation(document):\n",
    "    # This version of the function doesn't use stemming\n",
    "    words = document.split(' ')\n",
    "    words = clean_doc(words)\n",
    "    doc_len = len(words)\n",
    "    word_count = {}\n",
    "    for word in words :\n",
    "        word_count[word] = 0\n",
    "    for word in words :\n",
    "        word_count[word] += 1\n",
    "    \n",
    "    return word_count, doc_len\n",
    "\n",
    "def indexation_stem(document):\n",
    "    # This version of the function use stemming\n",
    "    words = document.split(' ')\n",
    "    words = clean_doc_stem(words)\n",
    "    doc_len = len(words)\n",
    "    word_count = {}\n",
    "    for word in words :\n",
    "        word_count[word] = 0\n",
    "    for word in words :\n",
    "        word_count[word] += 1\n",
    "    \n",
    "    return word_count, doc_len\n",
    "    \n",
    "\n",
    "def TF(w, document):\n",
    "    occurence = 0\n",
    "    if w in document[1]['word_count'].keys():\n",
    "        occurence = document[1]['word_count'][w]\n",
    "    return occurence/document[1]['doc_len']\n",
    "\n",
    "def iDF(numdoc, w, document):\n",
    "    #division par 0 impossible\n",
    "    occurence = 1\n",
    "    if w in document[1]['word_count'].keys():\n",
    "        occurence += 1\n",
    "    return np.log(numdoc/occurence)\n",
    "\n",
    "def tf_idf(query, table, k):\n",
    "    #Prends une requete sous la forme d'une liste de mots, un jeu de document et un nombre k pour renvoyer les k premiers resultats\n",
    "    tf_idf_value = {}\n",
    "    numdoc = len(table.keys())\n",
    "    for doc in table.items() :\n",
    "        tfidf_value = 0\n",
    "        for word in query :\n",
    "            tf = TF(word, doc)\n",
    "            idf = iDF(numdoc, word, doc)\n",
    "            tfidf_value += tf*idf\n",
    "        tf_idf_value[doc[0]] = tfidf_value\n",
    "    tf_idf_value_list = sorted(tf_idf_value.items(), key=lambda x: x[1], reverse=True)\n",
    "    return tf_idf_value_list[:k]\n",
    "\n",
    "def loadAll(filename, stem) :\n",
    "    #Fonction de parsing du fichier CISI.ALL\n",
    "    word_set = []\n",
    "    cisi_all = {}\n",
    "    id = 0\n",
    "    lines = open(filename).read()\n",
    "    for document in lines.split(\".I \") :\n",
    "        token = \"\"\n",
    "        buffer = \"\"\n",
    "        cross_reference = []\n",
    "        cisi_all[id] = {}\n",
    "        cisi_all[id]['ID'] = id\n",
    "        cisi_all[id]['titre'] = \"\"\n",
    "        cisi_all[id][\"texte\"] = \"\"\n",
    "        document += \"EndOfDocument\"\n",
    "        for line in document.split(\"\\n\") :\n",
    "            if token == \".T\" and line != \".A\":\n",
    "                cisi_all[id]['titre'] = line.replace(\"\\n\", \" \")\n",
    "            elif token == \".W\" and line != \".X\" :\n",
    "                buffer += \" \" + line.replace(\"\\n\", \" \")\n",
    "                continue\n",
    "            elif line == \".X\":\n",
    "                cisi_all[id][\"texte\"] = buffer\n",
    "                buffer = \"\"\n",
    "            elif token == \".X\" and line != \"EndOfDocument\":\n",
    "                cross_reference.append(line.split(\"\\t\"))\n",
    "                continue\n",
    "            elif line == \"EndOfDocument\":\n",
    "                cisi_all[id][\"cross_ref\"] = cross_reference\n",
    "                cross_reference = []\n",
    "            token = line\n",
    "        if(stem == True):\n",
    "            cisi_all[id]['word_count'], cisi_all[id]['doc_len'] = indexation_stem(cisi_all[id][\"texte\"] + \" \" + cisi_all[id]['titre'])\n",
    "        else :\n",
    "            cisi_all[id]['word_count'], cisi_all[id]['doc_len'] = indexation(cisi_all[id][\"texte\"] + \" \" + cisi_all[id]['titre'])\n",
    "        for word in cisi_all[id]['word_count'].keys():\n",
    "            if word not in word_set :\n",
    "                word_set.append(word)\n",
    "        id += 1\n",
    "    del cisi_all[0]\n",
    "    return cisi_all\n",
    "\n",
    "def loadQuery(filename, stem) :\n",
    "    #Fonction de parsing du fichier CISI.QRY \n",
    "    cisi_qry = {}\n",
    "    id = 0\n",
    "    lines = open(filename).read()\n",
    "    for document in lines.split(\".I \") :\n",
    "        token = \"\"\n",
    "        buffer = \"\"\n",
    "        cisi_qry[id] = {}\n",
    "        cisi_qry[id]['ID'] = id\n",
    "        cisi_qry[id][\"query\"] = \"\"\n",
    "        document += \"EndOfDocument\"\n",
    "        for line in document.split(\"\\n\") :\n",
    "            if token == \".W\" and line != \"EndOfDocument\" :\n",
    "                buffer += \" \" + line.replace(\"\\n\", \" \")\n",
    "                continue\n",
    "            elif line == \"EndOfDocument\":\n",
    "                cisi_qry[id][\"query\"] = buffer\n",
    "                buffer = \"\"\n",
    "                continue\n",
    "            token = line\n",
    "        if(stem == True):\n",
    "            cisi_qry[id]['query'], cisi_qry[id]['doc_len'] = indexation_stem(cisi_qry[id][\"query\"])\n",
    "        else :\n",
    "            cisi_qry[id]['query'], cisi_qry[id]['doc_len'] = indexation(cisi_qry[id][\"query\"])\n",
    "        id += 1\n",
    "    del cisi_qry[0]\n",
    "    return cisi_qry\n",
    "\n",
    "def loadRel(filename) : \n",
    "    #Fonction de parsing du fichier CISI.REL \n",
    "    cisi_rel = {}\n",
    "    lines = open(filename).read()\n",
    "    lines += \"EndOfDocument\"\n",
    "    buffer = 1\n",
    "    list_buffer = []\n",
    "    for line in lines.split(\"\\n\") :\n",
    "        if(line == \"EndOfDocument\"):\n",
    "            cisi_rel[token] = list_buffer\n",
    "            return cisi_rel\n",
    "        else :\n",
    "            token = int((line[0:6]).replace(\" \",\"\"))\n",
    "            if(token == buffer):\n",
    "                list_buffer.append(int((line[7:13]).replace(\" \",\"\")))\n",
    "                buffer = token\n",
    "                continue\n",
    "            else :\n",
    "                cisi_rel[buffer] = list_buffer\n",
    "                list_buffer = []\n",
    "                list_buffer.append(int((line[7:13]).replace(\" \",\"\")))\n",
    "                buffer = token\n",
    "                continue\n",
    "\n",
    "def tf_idf_evaluation_without_repetition(stem):\n",
    "    #Calcul de la precision moyenne global du modele sans poids associe aux mots. Stem est un booleen qui definit si nous utilisons du stemming ou non\n",
    "    final_list = []\n",
    "    QueryDic = loadQuery(\"CISI.QRY\", stem)\n",
    "    AllDic = loadAll(\"CISI.ALL\", stem)\n",
    "    RelDic = loadRel(\"CISI.REL\")\n",
    "    for query in QueryDic.items():\n",
    "        id = query[1][\"ID\"]\n",
    "        wordlist = []\n",
    "        for word in query[1][\"query\"].keys():\n",
    "            wordlist.append(word)\n",
    "        if id in RelDic.keys():\n",
    "            num_pages = len(RelDic[id])\n",
    "        else :\n",
    "            num_pages = 10\n",
    "        result = tf_idf(wordlist, AllDic, num_pages)\n",
    "        if id in RelDic.keys():\n",
    "            somme = 0\n",
    "            for pair in result :\n",
    "                if pair[0] in RelDic[id]:\n",
    "                    somme += 1\n",
    "            final_list.append(somme/num_pages)\n",
    "    return sum(final_list)/len(final_list)\n",
    "\n",
    "def tf_idf_evaluation_with_repetition(stem, ten):\n",
    "    #Calcul de la precision moyenne global du modele avec les poids associe aux mots. Stem est un booleen qui definit si nous utilisons du stemming ou non.\n",
    "    #Ten est un booleen dans le cadre du calcul de Precision@10, voir rapport.\n",
    "    final_list = []\n",
    "    QueryDic = loadQuery(\"CISI.QRY\", stem)\n",
    "    AllDic = loadAll(\"CISI.ALL\", stem)\n",
    "    RelDic = loadRel(\"CISI.REL\")\n",
    "    for query in QueryDic.items():\n",
    "        id = query[1][\"ID\"]\n",
    "        wordlist = []\n",
    "        for word in query[1][\"query\"].items():\n",
    "            i = 0\n",
    "            while(i < word[1]):\n",
    "                wordlist.append(word[0])\n",
    "                i += 1\n",
    "        if ten == False :\n",
    "            if id in RelDic.keys():\n",
    "                num_pages = len(RelDic[id])\n",
    "            else :\n",
    "                num_pages = 10\n",
    "        else :\n",
    "            num_pages = 10\n",
    "        result = tf_idf(wordlist, AllDic, num_pages)\n",
    "        if id in RelDic.keys():\n",
    "            somme = 0\n",
    "            for pair in result :\n",
    "                if ten == False :\n",
    "                    if pair[0] in RelDic[id]:\n",
    "                        somme += 1\n",
    "                else :\n",
    "                    if pair[0] in RelDic[id] and len(RelDic[id]) > 9 :\n",
    "                        somme += 1\n",
    "            final_list.append(somme/num_pages)\n",
    "    return sum(final_list)/len(final_list)\n",
    "\n",
    "\n",
    "\n",
    "def tf_idf_given_query(queryID, stem):\n",
    "    #Renvoie les 10 premiers resultats associes a un numero de requete donne\n",
    "    querytest = []\n",
    "    QueryDic = loadQuery(\"CISI.QRY\", stem)\n",
    "    for word in QueryDic[queryID][\"query\"].keys():\n",
    "            querytest.append(word)    \n",
    "    return tf_idf(querytest, loadAll(\"CISI.ALL\", stem), 10)\n",
    "\n",
    "print(\"\\n--- TFIDF sans repetition sans stem ---\\n\")\n",
    "print(tf_idf_evaluation_without_repetition(False))\n",
    "print(\"\\n--- TFIDF avec repetition sans stem ---\\n\")\n",
    "print(tf_idf_evaluation_with_repetition(False, False))\n",
    "print(\"\\n--- TFIDF sans repetition avec stem ---\\n\")\n",
    "print(tf_idf_evaluation_without_repetition(True))\n",
    "print(\"\\n--- TFIDF avec repetition avec stem ---\\n\")\n",
    "print(tf_idf_evaluation_with_repetition(True, False))\n",
    "print(\"\\n--- TFIDF avec repetition avec stem qu'avec les 10 ---\\n\")\n",
    "print(tf_idf_evaluation_with_repetition(True, True))\n",
    "print(\"\\n--- TFIDF query 6 avec stem ---\\n\")\n",
    "print(tf_idf_given_query(6, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': 1, 'titre': '18 Editions of the Dewey Decimal Classifications', 'texte': \"    The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad.\", 'cross_ref': [['1', '5', '1'], ['92', '1', '1'], ['262', '1', '1'], ['556', '1', '1'], ['1004', '1', '1'], ['1024', '1', '1'], ['1024', '1', '1']], 'word_count': {'present': 1, 'studi': 1, 'histori': 2, 'dewey': 3, 'decim': 2, 'classif': 2, 'first': 2, 'edit': 4, 'ddc': 2, 'publish': 1, '1876': 1, 'eighteenth': 1, '1971': 1, 'futur': 1, 'continu': 1, 'appear': 1, 'need': 1, 'spite': 1, 'long': 1, 'healthi': 1, 'life': 1, 'howev': 1, 'full': 1, 'stori': 1, 'never': 1, 'told': 1, 'biographi': 1, 'briefli': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'growth': 1, 'librarianship': 1, 'countri': 1, 'abroad': 1, '18': 1}, 'doc_len': 50}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfb5a87c35bcd43997975c71900d67fd988e3d78f446c88335b459b4ff2da931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
